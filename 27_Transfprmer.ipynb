{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9BVtomiEy7gdU1g15sUSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/27_Transfprmer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "The Transformer is a powerful deep learning model architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. It revolutionized natural language processing (NLP) tasks, especially machine translation, by eliminating the need for recurrent neural networks (RNNs) and introducing the attention mechanism as the primary mechanism for capturing dependencies in sequences.\n",
        "\n",
        "The Transformer model consists of an encoder-decoder architecture with attention mechanisms. It does not use recurrent layers like LSTM or GRU; instead, it relies heavily on self-attention layers, which allow it to capture long-range dependencies more effectively.\n",
        "\n",
        "## Impact of transformer\n",
        "1. Revolution in NLP\n",
        "2. Decomratizing AI refers to making artificial intelligence (AI) tools more accessible and usable for a wider range of users\n",
        "3. Multimodel capabilities\n",
        "4. Accerleration of GenAI\n",
        "5. Unification of DL"
      ],
      "metadata": {
        "id": "dVxTFHsG3AvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention is all you need\n",
        "\n",
        "In this paper, researcher proposed an architecture which is kind of encoder decoder architecture. But it does not use any LSTM, GRU. Whole architecture is based on self attention. It can do parallel training. Its hyperparameters are stable and make it robust."
      ],
      "metadata": {
        "id": "gdjZhASh7fXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timeline\n",
        "* 2000-2014 → RNN,LSTMs\n",
        "* 2014 → Attention\n",
        "* 2017 → Transformer\n",
        "* 2018 → BERT, GPT (transfer learning)\n",
        "* 2018-2020 → vison transformers (ALphaFord2)\n",
        "* 2021 → GenAI\n",
        "* 2022 → ChatGPT/ Stable Diffusion"
      ],
      "metadata": {
        "id": "Klbg8khk8Yb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages\n",
        "* Scalability\n",
        "* Transfer Learning\n",
        "* Multimodel input/output\n",
        "* Flexible architecture\n",
        "* Ecosystem\n",
        "* Integrated to other ai model"
      ],
      "metadata": {
        "id": "AIJEfaie9cuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT\n",
        "\n",
        "ChatGPT is the conversational version of the GPT (Generative Pre-trained Transformer) model developed by OpenAI. GPT is a type of deep learning model based on the Transformer architecture.\n",
        "\n",
        "## DALLE-E 2\n",
        "\"DALL·E 2\" refers to an advanced version or iteration of the DALL·E model developed by OpenAI. DALL·E is a generative model that can create diverse and realistic images from textual descriptions. The name \"DALL·E\" is a play on the name of the artist Salvador Dalí and the Pixar character Wall-E, indicating its ability to create surreal and imaginative images.\n",
        "\n",
        "## AlphaFolf\n",
        "AlphaFold is an artificial intelligence program developed by DeepMind, which is a subsidiary of Alphabet Inc. (the parent company of Google). AlphaFold specializes in predicting the 3D structure of proteins based on their amino acid sequences. It has gained significant attention and recognition for its accuracy and effectiveness in the field of computational biology and bioinformatics.\n",
        "\n",
        "## OpenAI Codex\n",
        "OpenAI Codex is an artificial intelligence model developed by OpenAI. It parses natural language and generates code in response. It powers GitHub Copilot, a programming autocompletion tool for select IDEs, like Visual Studio Code and Neovim.\n",
        "\n",
        "\n",
        "\n",
        "paper: [A comprehensive survey on applications of transformers for deep learning tasks](https://arxiv.org/pdf/2306.07303)\n",
        "\n"
      ],
      "metadata": {
        "id": "Pd9WeCEv99nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disadvantages\n",
        "* Requires high computational cost\n",
        "* Data, a lot of data\n",
        "* Overfitting\n",
        "* Energy consumptions\n",
        "* Interpretability\n",
        "* Bais"
      ],
      "metadata": {
        "id": "PRWOV-nz_w-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future\n",
        "* Improvement in efficency\n",
        "* Improve multimodel capabilities\n",
        "* Responsible training\n",
        "* Domain Specific\n",
        "* Multilingual\n",
        "* Interoperatibility"
      ],
      "metadata": {
        "id": "jMwHKM6bAPHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYx55eWP0I7A"
      },
      "outputs": [],
      "source": []
    }
  ]
}