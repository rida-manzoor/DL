{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5ug9MEMWMNvjcRzgMc/P7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/Loss_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is loss function**\n",
        "\n",
        "It is a method of evaluating how well your algorithm is modelling your dataset. If output of loss function is high, model is not bahaving good. It should be low."
      ],
      "metadata": {
        "id": "dOnFEyar-Pne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function in DL**\n",
        "Some main types of loss functions are:\n",
        "\n",
        "1. **Regression**\n",
        "\n",
        "    1. MSE\n",
        "    2. MAE\n",
        "    3. Huber loss\n",
        "2. **Classification**\n",
        "\n",
        "    1. Binary Crossentropy\n",
        "    2. Categorical Crossentropy\n",
        "    3. Hinge Loss\n",
        "\n",
        "3. **AutoEncoder**\n",
        "    1. KL Diverfence\n",
        "\n",
        "4. **GAN**\n",
        "    1. Discriminator loss\n",
        "    2. MinMax Gan loss\n",
        "\n",
        "5. **Embeddings**\n",
        "    1. Tripled Loss\n",
        "\n",
        "6. **Object Detection**\n",
        "    1. Focal Loss"
      ],
      "metadata": {
        "id": "DISg21bcABWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function vs Cost Function**\n",
        "\n",
        ">  **Loss function:** Used when we refer to the error for a single training example. **Cost function:** Used to refer to an average of the loss functions over an entire training data."
      ],
      "metadata": {
        "id": "tAQBaqd2B-At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error / L2 error**\n",
        "\n",
        "$$ (y_i - ̂y_i)^2  $$\n",
        "\n",
        "*Advantage*\n",
        "1. Easy to interpret\n",
        "2. Always differentiable\n",
        "3. One local minima\n",
        "\n",
        "*Disadvantage*\n",
        "1. Error unit is squared\n",
        "2. It is not robust to outliers\n",
        "\n",
        "> Activation function of last neuron should be linear"
      ],
      "metadata": {
        "id": "NaoglcYkCsAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute error / L1 error**\n",
        "\n",
        "$$ |y_i - ŷ_i| $$\n",
        "\n",
        "\n",
        "*Advantages*\n",
        "1. Easy to understand\n",
        "2. Unit is same as 'y'\n",
        "3. Robust to outliers\n",
        "\n",
        "*Disadvantages*\n",
        "1. Not differentiable (have to calculate sub-gradients)"
      ],
      "metadata": {
        "id": "r8YrWyAjD6uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Huber Loss**\n",
        "\n",
        " $$ {\\displaystyle L_{\\delta }(a)={\\begin{cases}{\\frac {1}{2}}{y-ŷ}&{\\text{for }}|y-ŷ|\\leq \\delta ,\\\\\\delta \\cdot \\left(|y-ŷ |-{\\frac {1}{2}}\\delta \\right),&{\\text{otherwise.}}\\end{cases}}}  $$\n",
        "\n",
        "If datapoint is outlier, huber will behave like MAE, if it is not outlier, huber will behave like MSE. **Sigma** is hyperparameter which will determine either datapoint is outlier or not.\n"
      ],
      "metadata": {
        "id": "bxepOl9CEnoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Crossentropy / log Loss**\n",
        "\n",
        "$$-ylog(ŷ) -(1-y)log(1-ŷ)  $$\n",
        "\n",
        "> Last activation function should be Sigmoid.\n",
        "\n",
        "*Advantage*\n",
        "1. Differentiable\n",
        "\n",
        "*Disadvantage*\n",
        "1. Multiple Local minima\n",
        "2. Not Intuitive"
      ],
      "metadata": {
        "id": "lG3EqHGbGO6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical crossentropy**\n",
        "\n",
        "$$ - ∑ y_j log ŷ_j $$\n",
        "\n",
        "> Activation fun should be softmax."
      ],
      "metadata": {
        "id": "CoGqZAq_G_Eu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOLhp3s0-IQw"
      },
      "outputs": [],
      "source": []
    }
  ]
}