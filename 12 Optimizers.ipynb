{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimizers**\n",
        "An optimizer is a function or an algorithm that adjusts the attributes of the neural network, such as weights and learning rates. Thus, it helps in reducing the overall loss and improving accuracy.\n",
        "\n",
        "## **Types of optimizers**\n",
        "- Batch GD\n",
        "- Mini Batch GD\n",
        "- Stochastic GD\n",
        "- Momentum\n",
        "- AdaGrad\n",
        "- NAG\n",
        "- Adam\n",
        "- RMSProp\n",
        "### Why we need other optimizers when we have these?\n",
        "\n",
        "**Challenges**\n",
        "- Learning rate (SOlution: LR Scheduling)\n",
        "- You have to minimize multiple weights. For 10 weights you will have 10 directions to move in. And we can not have seperate LR for each weight and bais.\n",
        "- Local Minima\n",
        "- Saddle point\n"
      ],
      "metadata": {
        "id": "eiajiId9rSFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponentially Weighted Moving Average\n",
        "\n",
        "Is a technique using which you find trends in time series data.\n",
        "\n",
        "Used in:\n",
        "- Stock\n",
        "- Time series forcasting\n",
        "- Signal Process\n",
        "\n",
        "**How it work?**\n",
        "Every new data point have more weightage as compared to previous point. And every point weightage will reduce over time.\n",
        "\n",
        "$$ V_t = βV_(t-1) +(1-β)θ_t $$\n",
        "\n",
        "beta value is between 0 and 1\n",
        "\n",
        "\n",
        "|Date|Temp|\n",
        "|-------------|-------------|\n",
        "| 1 | 23|\n",
        "|2|12|\n",
        "|3|17|\n",
        "|4|19|\n",
        "\n",
        "Suppose beta is 0.9\n",
        "\n",
        "For $V_0 = θ_0$ Which is 23\n",
        "\n",
        "For $V_1$ :  $0.9*23 + 0.1(23) = 23 $\n",
        "\n",
        "For $V_2$ :  $0.9*23 + 0.1(12) = 21.9 $\n",
        "\n",
        "For $V_3$ :  $0.9*21.9 + 0.1(17) = 21.41 $\n",
        "\n",
        "For $V_4$ :  $0.9*21.41 + 0.1(19) = 21.16 $\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Impact of Beta value**\n",
        "\n",
        "If beta value is 0.9, it will behave as it is average of last ten days.\n",
        "if beta = 0.2\n",
        "$ β → \\frac{1}{1-β} → 10 $\n"
      ],
      "metadata": {
        "id": "ylF6pDtMuNlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SGD with Momentum**\n",
        "## **The Why**\n",
        "\n",
        "In GD there was three problems:\n",
        "- Noisy Grad\n",
        "- Consistent grad\n",
        "- High curvature\n",
        "\n",
        "Momentum address all three problems\n",
        "\n",
        "## **The What**\n",
        "\n",
        "Consider you are moving from point A to point B but you don't know where exactly is point B. You will people where is it, if every person you asked pointed in one direction you will speed up in that direction. If people give mix direction your speed will be slower. This is the whole idea of momentum Optimizer.\n",
        "\n",
        "![alr](https://files.codingninjas.in/article_images/nesterov-accelerated-gradient-3-1644853109.webp)\n",
        "\n",
        "[link](https://www.codingninjas.com/studio/library/nesterov-accelerated-gradient)\n",
        "\n",
        "## **The How (Maths)**\n",
        "\n",
        "In normal optimizer we update weights with following equation:\n",
        "$$w_(t+1) = w_t - ηΔ w_t $$\n",
        "\n",
        "In momentum we take a term v which we will called velocity and instead of derivation we use velocity.$$w_(t+1) = w_t - v_t $$\n",
        "\n",
        "\n",
        "Here $v_t$ is calculated as: $$v_t = β v_(t-1) + η∇w_t$$\n",
        "\n",
        "Here $β$ value is between 0 and 1. $v_(t-1)$ is velocity at previous step. And second term is learning rate times current gradient.\n",
        "\n",
        "\n",
        "You are using history of velocity as momentum.\n",
        "\n",
        "### ***Effect of Beta***\n",
        "Beta is called Decaying factor.\n",
        "If Beta value is zero. Momentum will behave like SGD.\n",
        "\n",
        "\n",
        "If Beta value is one. There is no decay.\n",
        "\n",
        "\n",
        "## **Problem with momentum**\n",
        "\n",
        "It can be slow as compared to next optimizers. But will be fast than SGD.\n"
      ],
      "metadata": {
        "id": "MAh3uGJvzUaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nesterov Accelerated Gradient (NAG)**\n",
        "\n",
        "![alr](https://files.codingninjas.in/article_images/nesterov-accelerated-gradient-4-1644853109.webp)\n",
        "\n",
        "[link](https://www.codingninjas.com/studio/library/nesterov-accelerated-gradient)\n",
        "\n",
        "$$w_(t+1) = w_t − v_t$$\n",
        "While calculating the $v_t$, We will include the look ahead gradient $(∇w_(la))$.\n",
        "$$v_t = β ·v_(t−1) + η∇w_(la)$$\n",
        "\n",
        "$∇w_(la)$ is calculated by:\n",
        "$$w_(la) = w_t − β · v_(t−1)$$\n",
        "\n",
        "This look-ahead gradient will be used in our update and will prevent overshooting.\n",
        "\n",
        "\n",
        "**Disadvantage**\n",
        "- You can get trapped in local minima\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "tf.keras.optimizers.SGD(\n",
        "                      learning_rate = 0.01,\n",
        "                      momentum = 0.0,\n",
        "                      nestoriv = Fasle,\n",
        "                      name = \"SGD\",\n",
        "                      **kwargs)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OPpY9Z77p_wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AdaGrad**\n",
        "- Adaptive Gradient\n",
        "- AdaGrad uses different learning rates for each feature.\n",
        "\n",
        "## AdaGrad work better in following cases:\n",
        "- Input feature have different scale\n",
        "- Features are sparse\n",
        "\n",
        "\n",
        "**Enlongated Bowl Problem**"
      ],
      "metadata": {
        "id": "qXK0h4jDV8UR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rm1X2o8rOG3"
      },
      "outputs": [],
      "source": []
    }
  ]
}
