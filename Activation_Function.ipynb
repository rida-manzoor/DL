{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO48VUkr7JA9+jMpKz6Kmim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/Activation_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Function\n",
        "\n",
        "In ANN, each neuron forms a weighted sum of it's inputs and passes the resulting scalar value through a function referred as an activation function/transfer function. If neuron has n inputs then the output or activation of neuron is:\n",
        "$$ a = g(w_1x_1+w_2x_2+w_ix_1 +b) $$\n",
        "\n",
        "This 'g' is the activation function\n",
        "\n",
        "An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations.\n",
        "\n",
        "\n",
        "### Why we use activation function?\n",
        "If activation functions are not used, model can not capture non-linear data\n",
        "\n",
        "## Ideal Activation function\n",
        "- Nonlinear\n",
        "- Differentiable\n",
        "- computationally inexpensive\n",
        "- zero centered (normalized)\n",
        "- Non-Saturated (If saturating, vanishing gradient problem)\n",
        "\n"
      ],
      "metadata": {
        "id": "xlUkU1QF2Y00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid Activation function\n",
        "\n",
        "![alt](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/sigmoid-activation-function-2.png)\n",
        "\n",
        "$$ Ïƒ(x) = \\frac{1}{1+e^(-x)}$$\n",
        "\n",
        "\n",
        "**Advantages**\n",
        "- Output is between [0,1], can be treated as probability\n",
        "- Nonlinear Function, can capture non linearity of data\n",
        "- Differentiable\n",
        "\n",
        "**Disadvantages**\n",
        "- Saturating Function\n",
        "- Non-zero Centered (Output of each neuron is either positive or negative. It is not zero centered,Not normalized. Training will be slow, convergance slow)\n",
        "- computationally expensive\n",
        "\n",
        "\n",
        "## Tanh Activiation Function\n",
        "\n",
        "- [-1,1]\n",
        "$$ f(x) = \\frac{e^x -e^(-x)}{e^x+e^(-x)} $$\n",
        "\n",
        "**Advantages**\n",
        "- Zero centered\n",
        "- Non linear\n",
        "- Differentiable\n",
        "\n",
        "**Disadvantages**\n",
        "- Saturating Function\n",
        "- COmputationally expensive"
      ],
      "metadata": {
        "id": "aMkDNfTXEY4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RELU Activation function\n",
        "\n",
        "$$ f(x) = max(0,x)$$\n",
        "\n",
        "**Advantages**\n",
        "- Non-Linear\n",
        "- Not Saturated in positive region\n",
        "- Computationally inexpensive\n",
        "- Convergence faster comparision of sigmoid and tanh\n",
        "\n",
        "**Disadvantages**\n",
        "- Not zero centered\n",
        "- Not completely differentiable(at zero)\n",
        "\n",
        "\n",
        "### Problem of ReLu\n",
        "\n",
        "Dying Relu Problem (Dead Neuron)\n",
        "\n",
        "The dying ReLU problem refers to the scenario when many ReLU neurons only output values of 0. The red outline below shows that this happens when the inputs are in the negative range."
      ],
      "metadata": {
        "id": "HSeDH3w_Ncyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42anUr7ewORi"
      },
      "outputs": [],
      "source": []
    }
  ]
}