{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa2Td7AafBdPYEWcQYoXcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/Activation_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Function\n",
        "\n",
        "In ANN, each neuron forms a weighted sum of it's inputs and passes the resulting scalar value through a function referred as an activation function/transfer function. If neuron has n inputs then the output or activation of neuron is:\n",
        "$$ a = g(w_1x_1+w_2x_2+w_ix_1 +b) $$\n",
        "\n",
        "This 'g' is the activation function\n",
        "\n",
        "An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations.\n",
        "\n",
        "\n",
        "### Why we use activation function?\n",
        "If activation functions are not used, model can not capture non-linear data\n",
        "\n",
        "## Ideal Activation function\n",
        "- Nonlinear\n",
        "- Differentiable\n",
        "- computationally inexpensive\n",
        "- zero centered (normalized)\n",
        "- Non-Saturated (If saturating, vanishing gradient problem)\n",
        "\n"
      ],
      "metadata": {
        "id": "xlUkU1QF2Y00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid Activation function\n",
        "\n",
        "![alt](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/sigmoid-activation-function-2.png)\n",
        "\n",
        "$$ σ(x) = \\frac{1}{1+e^(-x)}$$\n",
        "\n",
        "\n",
        "**Advantages**\n",
        "- Output is between [0,1], can be treated as probability\n",
        "- Nonlinear Function, can capture non linearity of data\n",
        "- Differentiable\n",
        "\n",
        "**Disadvantages**\n",
        "- Saturating Function\n",
        "- Non-zero Centered (Output of each neuron is either positive or negative. It is not zero centered,Not normalized. Training will be slow, convergance slow)\n",
        "- computationally expensive\n",
        "\n",
        "\n",
        "## Tanh Activiation Function\n",
        "\n",
        "- [-1,1]\n",
        "$$ f(x) = \\frac{e^x -e^(-x)}{e^x+e^(-x)} $$\n",
        "\n",
        "**Advantages**\n",
        "- Zero centered\n",
        "- Non linear\n",
        "- Differentiable\n",
        "\n",
        "**Disadvantages**\n",
        "- Saturating Function\n",
        "- COmputationally expensive"
      ],
      "metadata": {
        "id": "aMkDNfTXEY4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RELU Activation function\n",
        "\n",
        "$$ f(x) = max(0,x)$$\n",
        "\n",
        "**Advantages**\n",
        "- Non-Linear\n",
        "- Not Saturated in positive region\n",
        "- Computationally inexpensive\n",
        "- Convergence faster comparision of sigmoid and tanh\n",
        "\n",
        "**Disadvantages**\n",
        "- Not zero centered\n",
        "- Not completely differentiable(at zero)\n",
        "\n",
        "\n",
        "### **Problem of ReLu**\n",
        "\n",
        "Dying Relu Problem (Dead Neuron)\n",
        "\n",
        "The dying ReLU problem refers to the scenario when many ReLU neurons only output values of 0. The red outline below shows that this happens when the inputs are in the negative range.\n",
        "\n",
        "**Reasons**\n",
        "- High learning rate\n",
        "- High negative bais\n",
        "\n",
        "**Solutions**\n",
        "- Set low learning rate\n",
        "- High positive bais 0.01\n",
        "- Use ReLu variants"
      ],
      "metadata": {
        "id": "HSeDH3w_Ncyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.**Linear Variants**\n",
        "\n",
        "\n",
        "- Leky ReLu\n",
        "- Parametric ReLu\n",
        "\n",
        "## 2. **Non Linear Variant**\n",
        "- Elu\n",
        "- SeLu"
      ],
      "metadata": {
        "id": "oVlLKI8tsQWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leky ReLu**\n",
        "\n",
        "$$ f(x) = max(0.01*z,z) $$\n",
        "\n",
        "\n",
        "![alt](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.09.45_PM.png)\n",
        "\n",
        "[Source](https://paperswithcode.com/method/leaky-relu)\n",
        "\n",
        "**Advantages**\n",
        "- Non Saturated\n",
        "- Easily computed\n",
        "- No dying ReLu problem\n",
        "- Close to zero centered\n",
        "\n",
        "\n",
        "Why we only take 0.01????"
      ],
      "metadata": {
        "id": "GeA8SE9xsm_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parametric ReLu\n",
        "$$ f(x) = \\begin{cases}\n",
        "x & \\text{if $x>0$} \\\\\n",
        "ax & \\text{otherwise}\n",
        "\\end{cases} $$\n",
        "\n",
        "where 'a' is a trainable parameter. Depending on data, we compute 'a', it give flexibity."
      ],
      "metadata": {
        "id": "t_lqXe86ucI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ELu\n",
        "$$ f(x) = \\begin{cases}\n",
        "x & \\text{if $x>0$} \\\\\n",
        "α(e^z-1) & \\text{if $x<0$}\n",
        "\\end{cases} $$\n",
        "\n",
        "\n",
        "- always continuous and differentiable function\n",
        "\n",
        "$$ f`(x) = \\begin{cases}\n",
        "1 & \\text{if $x>0$} \\\\\n",
        "f(x)+1 & \\text{if $x<=0$}\n",
        "\\end{cases} $$\n",
        "\n",
        "**Advantages**\n",
        "- value is close to zero centered\n",
        "- Faster convergence\n",
        "- Better generalization\n",
        "- Always continuous and differentiable\n",
        "- No DYing relu problem\n",
        "\n",
        "\n",
        "**Disadvantages**\n",
        "- Computationally expensive"
      ],
      "metadata": {
        "id": "eZe3VwRFv4bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SeLu\n",
        "$$ f(x) = λ\\begin{cases}\n",
        "x & \\text{if $x>0$} \\\\\n",
        "α(e^z-1) & \\text{if $x<=0$}\n",
        "\\end{cases} $$\n",
        "\n",
        "Comparitively new so not that adopted\n",
        "\n",
        "**Advantage**\n",
        "- It is self normalizing So converge faster"
      ],
      "metadata": {
        "id": "XzINkqchxJZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42anUr7ewORi"
      },
      "outputs": [],
      "source": []
    }
  ]
}