{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcTo4FT2eFGQ9jbUk0WBSY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/DL/blob/main/22_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Short Term Memory(LSTM)\n",
        "\n",
        "\n",
        "\n",
        "*  Defination\n",
        "*  Problem with RNN\n",
        "*  Architecture\n",
        "\n",
        "* Reference\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JsM-8m0M5hHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM is a type of recurrent neural network (RNN) architecture designed to address the vanishing and exploding gradient problems in traditional RNNs. It is well-suited for processing and making predictions based on sequences of data, such as time series data, natural language, and more.\n",
        "\n",
        "\n",
        "LSTM networks were designed by Sepp Hochreiter and Jürgen Schmidhuber in 1997. They introduced LSTM in a paper titled \"[Long Short-Term Memory](https://blog.xpgreat.com/file/lstm.pdf)\" published in the journal *Neural Computation* (Volume 9, Issue 8, pages 1735-1780). This paper outlined the architecture and benefits of LSTM networks, highlighting their ability to overcome the vanishing gradient problem in traditional recurrent neural networks (RNNs) and handle long-range dependencies in sequential data."
      ],
      "metadata": {
        "id": "ITBgyQC9rDSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem with RNN\n",
        "\n",
        "The main problem with traditional Recurrent Neural Networks (RNNs) is the vanishing gradient problem. In RNNs, during training, gradients tend to either vanish (become extremely small) or explode (become extremely large) as they are back-propagated through time. This occurs because of the repeated multiplication of gradients through the weight matrices in the network during backpropagation.\n",
        "\n",
        "When gradients vanish, the model struggles to learn long-term dependencies in sequences. Essentially, information from earlier time steps becomes increasingly diluted or lost as it is propagated through time, making it difficult for the model to capture long-range dependencies. This limitation severely hampers the performance of RNNs in tasks where understanding context over longer sequences is crucial, such as natural language processing (NLP) tasks like language translation or sentiment analysis.\n",
        "\n",
        "![RNN](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png)\n",
        "\n",
        "As we know we have to assign weight to feedback loop.\n",
        "- COnsider if weight is 2 and we replicate that steps 70 times(Because we have 70 rows in stock prediction data)\n",
        "\n",
        " $$      weight^(70)   $$\n",
        "  $$      2^(70)   $$\n",
        "    so gradient will explode in this case.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In RNN, hidden state only retain past data"
      ],
      "metadata": {
        "id": "bRBppkHKr3c7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture\n",
        "\n",
        "LSTM has following three gates:\n",
        "1. Forget Gate\n",
        "2. Input Gate\n",
        "3. Output Gate\n",
        "\n",
        "Before we see it's architecture we should know what goes in as input and exactly we get as an output from a particular cell in LSTM architecture at a given time step.\n",
        "\n",
        "- Input:\n",
        "     * Previous Cell State\n",
        "     * Previous Hidden State\n",
        "     * Input from current timestep\n",
        "- Output\n",
        "     * Current Hidden State\n",
        "     * Current Cell State\n",
        "- |=|\n",
        "     * Update Cell State\n",
        "     * Calculate h_t\n",
        "\n",
        "![LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations. Number of nodes in each neural netwrok layer is hyperparameter. More over all vectors will be of same shape and dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "XCdQSpfLxWU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gates has the ability to add or delete some information from cell state.\n",
        "\n",
        "## 1. Forget Gate\n",
        "![forget Gate](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
        "\n",
        "Forget gate decides which information should be deleted from cell state.\n",
        "Assume we have 3 nodes of sigmoid neural network. And Input looks like:\n",
        "$$\n",
        "x_t = [x_i1, x_i2, x_i3, x_i4]\n",
        "$$\n",
        "\n",
        "$$c_t,h_t$$ dimension will be 3 same as no. of nodes.\n",
        "\n",
        "Now we have to caluclate:\n",
        "1. $$f_t$$\n",
        "2. $$c_(t-1) x f_t$$\n",
        "\n",
        "Where, we can say second step is removal operation. It decides which information to keep and which to delete. In this particular setup, we will have total 21 weights.\n",
        "\n",
        "$$f_t=[f_1,f_2,f_3]$$\n",
        "$$w=(3,7)$$\n",
        "\n",
        "Now, we will see how shape of all vectors is like:\n",
        "$$f_t=σ(w_f[h_t-1,x_t] +b_f)$$\n",
        "\n",
        "where\n",
        "$$\n",
        "w_f=(3,7)$$\n",
        "$$\n",
        "[h_t-1,x_t] =(7,1)\n",
        "$$\n",
        "where there product will be equal to (3,1)\n",
        "$$b_f=(3,1)$$\n",
        "$$(3,1)+(3,1)=(3,1)$$"
      ],
      "metadata": {
        "id": "1ETaJjasGUKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Gate\n",
        "This gate add some new input\n",
        "![Input](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
        "\n",
        "Here, we have to calculate:\n",
        "1. c_t-1 Candidate cell state\n",
        "2. i_t Fitering\n",
        "3. c_t current cell state\n",
        "\n"
      ],
      "metadata": {
        "id": "bw6dbxYaUPyv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnHcuRPxsbmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jB9aX_5sbkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFSUubAh43hb"
      },
      "outputs": [],
      "source": [
        "4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "- https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/\n",
        "- https://blog.xpgreat.com/file/lstm.pdf\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ],
      "metadata": {
        "id": "0ScEW47LvCX5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X9HjcY1LvTag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}